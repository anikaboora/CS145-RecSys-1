import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import shutil
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence

from pyspark.sql import SparkSession
from pyspark.sql import functions as sf
from pyspark.sql import DataFrame, Window
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.feature import StringIndexer, OneHotEncoder
from pyspark.ml.linalg import Vectors, VectorUDT
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from pyspark.sql.functions import col, lit
from pyspark.sql.functions import udf
from pyspark.ml.linalg import VectorUDT
from pyspark.sql.types import DoubleType, ArrayType
from pyspark.sql.functions import monotonically_increasing_id

class LogisticRecommender:
    """
    Logistic Recommender class
    """
    
    def __init__(self, seed=None):
        """
        Initialize recommender.
        
        Args:
            seed: Random seed for reproducibility
        """
        self.seed = seed
        # Add your initialization logic here
        self.model = None
        self.assembler = None

        self.segment_indexer = StringIndexer(inputCol="segment", outputCol="segment_index")
        self.category_indexer = StringIndexer(inputCol="category", outputCol="category_index")
        self.segment_encoder = OneHotEncoder(inputCol="segment_index", outputCol="segment_ohe")
        self.category_encoder = OneHotEncoder(inputCol="category_index", outputCol="category_ohe")
    
    def fit(self, log, user_features=None, item_features=None):
        """
        Train the recommender model based on interaction history.
        
        Args:
            log: Interaction log with user_idx, item_idx, and relevance columns
            user_features: User features dataframe (optional)
            item_features: Item features dataframe (optional)
        """
        # Implement your training logic here
        # For example:
        #  1. Extract relevant features from user_features and item_features
        #  2. Learn user preferences from the log
        #  3. Build item similarity matrices or latent factor models
        #  4. Store learned parameters for later prediction
        pos = log.withColumn("label", lit(1.0))

        # Sample negative interactions (user-item pairs not in the log)
        all_pairs = user_features.crossJoin(item_features)
        neg = all_pairs.join(log.select("user_idx", "item_idx"), 
                             on=["user_idx", "item_idx"], how="left_anti") \
                       .withColumn("label", lit(0.0)) \
                       .sample(False, 0.05, seed=self.seed)
        neg = neg.withColumn("relevance", lit(0.0))


        # Combine positive and negative examples
        pos = pos.join(user_features, on="user_idx", how="left")
        pos = pos.join(item_features, on="item_idx", how="left")
        # print("Positive DF schema:")
        # pos.printSchema()
        # print("Negative DF schema:")
        # neg.printSchema()

        if "__iter" in pos.columns:
            pos = pos.drop("__iter")

        data = pos.unionByName(neg)

        data = self.segment_indexer.fit(data).transform(data)
        data = self.category_indexer.fit(data).transform(data)
        data = self.segment_encoder.fit(data).transform(data)
        data = self.category_encoder.fit(data).transform(data)

        # Get all feature columns (excluding labels and indices)
        feature_cols = [col for col in data.columns if col not in ("user_idx", "item_idx", "label", "relevance", "segment", "category")]
        feature_cols += ["segment_ohe", "category_ohe"]
        
        # Assemble features
        self.assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
        data = self.assembler.transform(data)

        # Train logistic regression
        lr = LogisticRegression(featuresCol="features", labelCol="label", maxIter=10)
        self.model = lr.fit(data)
    
    def predict(self, log, k, users, items, user_features=None, item_features=None, filter_seen_items=True):
        """
        Generate recommendations for users.
        
        Args:
            log: Interaction log with user_idx, item_idx, and relevance columns
            k: Number of items to recommend
            users: User dataframe
            items: Item dataframe
            user_features: User features dataframe (optional)
            item_features: Item features dataframe (optional)
            filter_seen_items: Whether to filter already seen items
            
        Returns:
            DataFrame: Recommendations with user_idx, item_idx, and relevance columns
        """
        # Implement your recommendation logic here
        # For example:
        #  1. Extract relevant features for prediction
        #  2. Calculate relevance scores for each user-item pair
        #  3. Rank items by relevance and select top-k
        #  4. Return a dataframe with columns: user_idx, item_idx, relevance
        
        # Example of a random recommender implementation:
        # Cross join users and items
        recs = users.crossJoin(items)
        
        # Filter out already seen items if needed
        if filter_seen_items and log is not None:
            seen_items = log.select("user_idx", "item_idx")
            recs = recs.join(
                seen_items,
                on=["user_idx", "item_idx"],
                how="left_anti"
            )

        # print("Recs schema:")
        # recs.printSchema()

        # Assemble features
        recs = self.segment_indexer.fit(recs).transform(recs)
        recs = self.category_indexer.fit(recs).transform(recs)
        recs = self.segment_encoder.fit(recs).transform(recs)
        recs = self.category_encoder.fit(recs).transform(recs)

        recs = self.assembler.transform(recs)

        # Predict probabilities
        scored = self.model.transform(recs)
        scored = scored.withColumnRenamed("probability", "raw_prob")
        def extract_prob(v):
            return float(v[1]) if len(v) > 1 else 0.0

        extract_prob_udf = udf(extract_prob, DoubleType())
        scored = scored.withColumn("relevance", extract_prob_udf(col("raw_prob")))

        # Rank and filter top-k
        window = Window.partitionBy("user_idx").orderBy(col("relevance").desc())
        scored = scored.withColumn("rank", sf.row_number().over(window)) \
                       .filter(col("rank") <= k) \
                       .select("user_idx", "item_idx", "relevance")

        return scored

  
