import os
import sys
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from collections import defaultdict
import shutil
import lightgbm as lgb
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from torch.nn.utils.rnn import pad_sequence

from pyspark.sql import SparkSession
from pyspark.sql import functions as sf
from pyspark.sql import DataFrame, Window
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.feature import StringIndexer, OneHotEncoder
from pyspark.ml.classification import LogisticRegression
from pyspark.ml import Pipeline
from pyspark.sql.functions import col, lit
from pyspark.sql.functions import udf
from pyspark.ml.linalg import Vectors, VectorUDT
from pyspark.sql.types import DoubleType, ArrayType
from pyspark.sql.functions import monotonically_increasing_id
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics.pairwise import pairwise_distances

class LogisticRecommender:
    """
    Logistic Recommender class
    """
    
    def __init__(self, seed=None):
        """
        Initialize recommender.
        
        Args:
            seed: Random seed for reproducibility
        """
        self.seed = seed
        # Add your initialization logic here
        self.model = None
        self.assembler = None

        self.segment_indexer = StringIndexer(inputCol="segment", outputCol="segment_index")
        self.category_indexer = StringIndexer(inputCol="category", outputCol="category_index")
        self.segment_encoder = OneHotEncoder(inputCol="segment_index", outputCol="segment_ohe")
        self.category_encoder = OneHotEncoder(inputCol="category_index", outputCol="category_ohe")
    
    def fit(self, log, user_features=None, item_features=None):
        """
        Train the recommender model based on interaction history.
        
        Args:
            log: Interaction log with user_idx, item_idx, and relevance columns
            user_features: User features dataframe (optional)
            item_features: Item features dataframe (optional)
        """
        # Implement your training logic here
        # For example:
        #  1. Extract relevant features from user_features and item_features
        #  2. Learn user preferences from the log
        #  3. Build item similarity matrices or latent factor models
        #  4. Store learned parameters for later prediction
        pos = log.withColumn("label", lit(1.0))

        # Sample negative interactions (user-item pairs not in the log)
        all_pairs = user_features.crossJoin(item_features)
        neg = all_pairs.join(log.select("user_idx", "item_idx"), 
                             on=["user_idx", "item_idx"], how="left_anti") \
                       .withColumn("label", lit(0.0)) \
                       .sample(False, 0.05, seed=self.seed)
        neg = neg.withColumn("relevance", lit(0.0))


        # Combine positive and negative examples
        pos = pos.join(user_features, on="user_idx", how="left")
        pos = pos.join(item_features, on="item_idx", how="left")
        # print("Positive DF schema:")
        # pos.printSchema()
        # print("Negative DF schema:")
        # neg.printSchema()

        if "__iter" in pos.columns:
            pos = pos.drop("__iter")

        data = pos.unionByName(neg)

        data = self.segment_indexer.fit(data).transform(data)
        data = self.category_indexer.fit(data).transform(data)
        data = self.segment_encoder.fit(data).transform(data)
        data = self.category_encoder.fit(data).transform(data)

        # Get all feature columns (excluding labels and indices)
        feature_cols = [col for col in data.columns if col not in ("user_idx", "item_idx", "label", "relevance", "segment", "category")]
        feature_cols += ["segment_ohe", "category_ohe"]
        
        # Assemble features
        self.assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
        data = self.assembler.transform(data)

        # Train logistic regression
        lr = LogisticRegression(featuresCol="features", labelCol="label", maxIter=10)
        self.model = lr.fit(data)
    
    def predict(self, log, k, users, items, user_features=None, item_features=None, filter_seen_items=True):
        """
        Generate recommendations for users.
        
        Args:
            log: Interaction log with user_idx, item_idx, and relevance columns
            k: Number of items to recommend
            users: User dataframe
            items: Item dataframe
            user_features: User features dataframe (optional)
            item_features: Item features dataframe (optional)
            filter_seen_items: Whether to filter already seen items
            
        Returns:
            DataFrame: Recommendations with user_idx, item_idx, and relevance columns
        """
        # Implement your recommendation logic here
        # For example:
        #  1. Extract relevant features for prediction
        #  2. Calculate relevance scores for each user-item pair
        #  3. Rank items by relevance and select top-k
        #  4. Return a dataframe with columns: user_idx, item_idx, relevance
        
        # Example of a random recommender implementation:
        # Cross join users and items
        recs = users.crossJoin(items)
        
        # Filter out already seen items if needed
        if filter_seen_items and log is not None:
            seen_items = log.select("user_idx", "item_idx")
            recs = recs.join(
                seen_items,
                on=["user_idx", "item_idx"],
                how="left_anti"
            )

        # print("Recs schema:")
        # recs.printSchema()

        # Assemble features
        recs = self.segment_indexer.fit(recs).transform(recs)
        recs = self.category_indexer.fit(recs).transform(recs)
        recs = self.segment_encoder.fit(recs).transform(recs)
        recs = self.category_encoder.fit(recs).transform(recs)

        recs = self.assembler.transform(recs)

        # Predict probabilities
        scored = self.model.transform(recs)
        scored = scored.withColumnRenamed("probability", "raw_prob")
        def extract_prob(v):
            return float(v[1]) if len(v) > 1 else 0.0

        extract_prob_udf = udf(extract_prob, DoubleType())
        scored = scored.withColumn("relevance", extract_prob_udf(col("raw_prob")))

        # Rank and filter top-k
        window = Window.partitionBy("user_idx").orderBy(col("relevance").desc())
        scored = scored.withColumn("rank", sf.row_number().over(window)) \
                       .filter(col("rank") <= k) \
                       .select("user_idx", "item_idx", "relevance")

        return scored

class GradientBoosting:
    """
    Template class for implementing a custom recommender.
   
    This class provides the basic structure required to implement a recommender
    that can be used with the Sim4Rec simulator. Students should extend this class
    with their own recommendation algorithm.
    """
   
    def __init__(self, seed=None):
        """
        Initialize recommender.
       
        Args:
            seed: Random seed for reproducibility
        """
        self.seed = seed
        self.feature_cols = None
        self.model = None
        # Add your initialization logic here
   
    def fit(self, log, user_features=None, item_features=None):
        """
        Train the recommender model based on interaction history.
       
        Args:
            log: Interaction log with user_idx, item_idx, and relevance columns
            user_features: User features dataframe (optional)
            item_features: Item features dataframe (optional)
        """
        df = log

        if user_features is not None:
            df = df.join(user_features, on='user_idx', how='left')
        if item_features is not None:
            df = df.join(item_features, on='item_idx', how='left')

        pdf = df.toPandas()

        self.categorical_cols = pdf.select_dtypes(include=['category', 'object']).columns.tolist()
       
        self.feature_cols = [col for col in pdf.columns if col not in ['user_idx', 'item_idx', 'relevance']]

        pdf_sorted = pdf.sort_values(by='user_idx')
        X = pdf_sorted[self.feature_cols]
        y = pdf_sorted['relevance']
        group_sizes = pdf_sorted.groupby('user_idx').size().tolist()


        labelencoder = LabelEncoder()

        for col in self.categorical_cols:
            X[col] = labelencoder.fit_transform(X[col])

        for col in self.categorical_cols:
            X[col] = X[col].astype('int')

        self.model = lgb.LGBMRanker(
            objective='lambdarank',
            metric='ndcg',
            boosting_type='gbdt',
            n_estimators=1000,
            learning_rate=0.01,
            random_state=42
        )
        self.model.fit(X, y, group=group_sizes, categorical_feature=self.categorical_cols)
   
    def predict(self, log, k, users, items, user_features=None, item_features=None, filter_seen_items=True):
        """
        Generate recommendations for users.
       
        Args:
            log: Interaction log with user_idx, item_idx, and relevance columns
            k: Number of items to recommend
            users: User dataframe
            items: Item dataframe
            user_features: User features dataframe (optional)
            item_features: Item features dataframe (optional)
            filter_seen_items: Whether to filter already seen items
       
        Returns:
            DataFrame: Recommendations with user_idx, item_idx, and relevance columns
        """
        # Example of a random recommender implementation:
        # Cross join users and items
        # Generate all user-item combinations
        user_items = users.crossJoin(items)


        # Join with features
        if user_features is not None:
            user_items = user_items.join(user_features, on='user_idx', how='left')
        if item_features is not None:
            user_items = user_items.join(item_features, on='item_idx', how='left')


        if filter_seen_items and log is not None:
            # Remove already interacted user-item pairs
            seen = log.select('user_idx', 'item_idx').dropDuplicates()
            user_items = user_items.join(seen, on=['user_idx', 'item_idx'], how='left_anti')


        # Convert to pandas for prediction
        pdf = user_items.toPandas()
        pdf = pdf.loc[:, ~pdf.columns.duplicated()]


        # Keep original indices for result formatting
        original_user_item = pdf[['user_idx', 'item_idx']].copy()


        # Ensure all feature columns are present
        for col in self.feature_cols:
            if col not in pdf.columns:
                pdf[col] = None  # or fill with a default value


        # Encode categorical features using LabelEncoder
        for col in self.categorical_cols:
            pdf[col] = pdf[col].astype(str)  # ensure consistent dtype
            pdf[col] = LabelEncoder().fit_transform(pdf[col])  # Fit anew or reuse encoder if saved


        X = pdf[self.feature_cols]


        # Predict relevance scores
        preds = self.model.predict(X)


        # Attach predictions to user-item pairs
        result_df = original_user_item.copy()
        result_df['relevance'] = preds


        # Return top-k per user
        result_df = (
            result_df.sort_values(['user_idx', 'relevance'], ascending=[True, False])
            .groupby('user_idx')
            .head(k)
            .reset_index(drop=True)
        )


        return spark.createDataFrame(result_df)


class KNNRecommender:
    def __init__(self, k=10, metric='cosine', user_based=True, seed=None):
        """
        Flexible KNN Recommender

        Args:
            k: number of neighbors
            metric: distance metric ('cosine', 'euclidean', 'manhattan')
            user_based: if True, use user-user similarity; else item-item similarity
            seed: random seed
        """
        self.k = k
        self.metric = metric
        self.user_based = user_based
        self.seed = seed

    def fit(self, log, user_features=None, item_features=None):
        """
        Prepare scaled feature matrices and index mappings
        """
        self.log_df = log.toPandas()
        self.user_df = user_features.toPandas().set_index("user_idx")
        self.item_df = item_features.toPandas().set_index("item_idx")

        # Select and normalize numerical features
        self.user_vecs = StandardScaler().fit_transform(self.user_df.filter(like='user_attr_'))
        self.item_vecs = StandardScaler().fit_transform(self.item_df.filter(like='item_attr_'))

        self.user_ids = self.user_df.index.tolist()
        self.item_ids = self.item_df.index.tolist()

        # Create price lookup
        self.price_lookup = self.item_df["price"].to_dict()

    def predict(self, log, k, users, items, user_features=None, item_features=None, filter_seen_items=True):
        """
        Recommend items to each user based on similarity and price Ã— similarity scoring
        """
        users_pd = users.toPandas()
        items_pd = items.toPandas()

        # Candidate items per user
        candidate_map = {int(uid): set(items_pd["item_idx"].tolist()) for uid in users_pd["user_idx"]}

        # Filter out previously seen items
        if filter_seen_items and log is not None:
            seen_df = log.select("user_idx", "item_idx").toPandas()
            for _, row in seen_df.iterrows():
                uid = int(row["user_idx"])
                if uid in candidate_map:
                    candidate_map[uid].discard(row["item_idx"])

        recs = []

        for uid in users_pd["user_idx"]:
            uid = int(uid)
            if uid not in self.user_ids:
                continue

            if self.user_based:
                # User-based: find similar users
                query_vec = self.user_vecs[self.user_ids.index(uid)].reshape(1, -1)
                target_matrix = self.user_vecs
                targets = self.user_ids
            else:
                # Item-based: same query, but compare to items
                query_vec = self.user_vecs[self.user_ids.index(uid)].reshape(1, -1)
                target_matrix = self.item_vecs
                targets = self.item_ids

            # Compute similarities
            distances = pairwise_distances(query_vec, target_matrix, metric=self.metric).flatten()
            similarities = 1 / (distances + 1e-8)  # Avoid division by zero

            # Get top-k neighbors
            top_k_indices = np.argsort(similarities)[-self.k:]
            top_k = [(targets[i], similarities[i]) for i in top_k_indices]

            # Score items
            scored = {}

            if self.user_based:
                for neighbor_uid, sim in top_k:
                    neighbor_logs = self.log_df[self.log_df["user_idx"] == neighbor_uid]
                    for _, row in neighbor_logs.iterrows():
                        item = row["item_idx"]
                        if item in candidate_map[uid]:
                            price = self.price_lookup.get(item, 0)
                            scored[item] = scored.get(item, 0) + sim * price
            else:
                for item in candidate_map[uid]:
                    item_idx = self.item_ids.index(item)
                    sim = similarities[item_idx]
                    price = self.price_lookup.get(item, 0)
                    scored[item] = sim * price

            # Rank and store top-k items
            top_items = sorted(scored.items(), key=lambda x: x[1], reverse=True)[:k]
            for item_id, score in top_items:
                recs.append((int(uid), int(item_id), float(score)))

        # Convert to PySpark DataFrame
        from pyspark.sql import SparkSession
        spark = SparkSession.builder.getOrCreate()
        return spark.createDataFrame(recs, ["user_idx", "item_idx", "relevance"])
  
